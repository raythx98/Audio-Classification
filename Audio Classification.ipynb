{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Audio Classification.ipynb","provenance":[],"collapsed_sections":["Tz-eN4RZTvyW","5CD5HBKjCA3m","E3_rz0d4Khpn","Q9hhlhDDMEos","3OWoxpWcM3D4","rUGlO1lFRQQd","qeGHrPzIa9_7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CtLPLbA0cxBE"},"source":["## Downloading Dependencies"]},{"cell_type":"code","metadata":{"id":"2h7Cxnb-5XHg"},"source":["# install torchaudio\n","!pip install torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3e-YTe8ZK0Ak"},"source":["import os\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5AjX3qrmvj0x"},"source":["import random\n","import librosa\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# current torch version is 1.7.0+cu101\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from torchaudio.transforms import MelSpectrogram\n","from torchaudio.transforms import TimeMasking\n","from torchaudio.transforms import FrequencyMasking\n","from torchaudio.transforms import TimeStretch\n","\n","import torchaudio\n","\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUiwwAIP6_o_"},"source":["# check if cuda GPU is available, make sure you're using GPU runtime on Google Colab\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device) # you should output \"cuda\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lEKJfxZjPX8"},"source":["## Importing dataset from Google Drive\n","\n"]},{"cell_type":"code","metadata":{"id":"yL0YTfKljK-9"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","base_folder = '/content/drive/MyDrive' # Change this path to your desired directory\n","training_path = os.path.join(base_folder, \"training_dataset.zip\") # zip file for training (training + validation) dataset\n","\n","!unzip $training_path\n","\n","test_path = os.path.join(base_folder, \"test_dataset.zip\") # zip file for training dataset\n","!unzip $test_path"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9z4qZ8A8HTn"},"source":["## Speech Classification Dataset"]},{"cell_type":"code","metadata":{"id":"j8Iv0DXlINyH"},"source":["class CustomSpeechDataset(torch.utils.data.Dataset):\n","  def __init__(self, path, typ='train', transforms=None):\n","\n","    assert typ == 'train' or typ == 'test', 'typ must be either \"train\" or \"test\"'\n","\n","    self.typ = typ\n","    self.transforms = transforms\n","    self.targets = []\n","\n","    if self.typ == 'train':\n","      self.class_names = sorted(os.listdir(path))\n","      num_classes = len(self.class_names)\n","\n","      for class_idx, class_name in enumerate(self.class_names):\n","        class_dirx = os.path.join(path, class_name)\n","        wav_list = os.listdir(class_dirx)\n","\n","        for wav_file in wav_list:\n","          self.targets.append({\n","              'filename': wav_file,\n","              'path': os.path.join(class_dirx, wav_file),\n","              'class': class_name\n","          })\n","\n","    if self.typ == 'test':\n","      wav_list = os.listdir(path)\n","      for wav_file in wav_list:\n","        self.targets.append({\n","            'filename': wav_file,\n","            'path': os.path.join(path, wav_file)\n","        })\n","  \n","  def __len__(self):\n","    return len(self.targets)\n","\n","  def __getitem__(self, idx):\n","    if torch.is_tensor(idx):\n","      idx.tolist()\n","\n","    signal, sr = torchaudio.load(self.targets[idx]['path'], normalization=True)\n","    filename = self.targets[idx]['filename']\n","\n","    if self.transforms:\n","      for transform in self.transforms:\n","        signal = transform(signal)\n","\n","    if self.typ == 'train':\n","      clx_name = self.targets[idx]['class']\n","      return filename, signal, sr, clx_name\n","    \n","    elif self.typ == 'test':\n","      return filename, signal, sr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yCUkL1VKtTH"},"source":["full_dataset = CustomSpeechDataset(path='training_dataset', typ='train') # Change to the folder inside the zip file\n","train_size = int(len(full_dataset)*0.8)\n","valid_size = len(full_dataset) - train_size\n","train_set, valid_set = torch.utils.data.random_split(full_dataset, [train_size, valid_size])\n","labels = full_dataset.class_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uio7QFjHaDoe"},"source":["labels_to_indices = {}\n","for idx, l in enumerate(labels):\n","  labels_to_indices[l] = idx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jrrtzO7KBiq4"},"source":["Let's next look at one example from the training set."]},{"cell_type":"code","metadata":{"id":"IgwfKIIl_P5k"},"source":["filename, waveform, sample_rate, label_id = train_set[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmL1bsGlALvM"},"source":["print(\"Shape of waveform: {}\".format(waveform.size()))\n","print(\"Sample rate of waveform: {}\".format(sample_rate))\n","\n","# We can plot the waveform using matplotlib\n","plt.plot(waveform.t().numpy());"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBhtVJs5Ao2s"},"source":["# We can play the audio clip and hear it for ourselves!\n","ipd.Audio(waveform.numpy(), rate=sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tz-eN4RZTvyW"},"source":["## Audio Augmentation"]},{"cell_type":"markdown","metadata":{"id":"seKEy9j6yK5t"},"source":["To make our model more robust and less susceptible to overfitting of training instances, we can make random changes to make it different in each epoch."]},{"cell_type":"code","metadata":{"id":"P5t38-Zll-Wd"},"source":["# let's set up a list of transformations we are going to apply to the waveforms\n","transformations = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52DleXLemDA3"},"source":["### Add Noise to Audio\n"]},{"cell_type":"code","metadata":{"id":"Dy3ax71Rl98z"},"source":["class AddNoise(torch.nn.Module):\n","  def __init__(self, noise_amt = 0.1):\n","    super().__init__()\n","    self.noise_amt = noise_amt\n","\n","  def forward(self, waveform):\n","    waveform += np.random.normal(0, self.noise_amt*torch.max(waveform), waveform.size())\n","    return waveform\n","\n","transformations.append(AddNoise(noise_amt = 0.01))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2Fqr_Qpmgmp"},"source":["### Convert Tensor to Numpy\n","We can simplify augmentation operations using Numpy operations"]},{"cell_type":"code","metadata":{"id":"VlYAXgxfm2mX"},"source":["class toNumpy(torch.nn.Module):\n","    def __init__(self, log_offset = 1e-6):\n","        super().__init__()\n","\n","    def forward(self, waveform):\n","        return waveform[0].numpy();\n","        \n","transformations.append(toNumpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o8ISBUIMnUCx"},"source":["### Time Warping\n","We can perform a time warp by shifting the audio to the left or right"]},{"cell_type":"code","metadata":{"id":"z9t__wHMnJoE"},"source":["class AddShift(torch.nn.Module):\n","  def __init__(self, max_shift = 1600):\n","    super().__init__()\n","    self.max_shift = max_shift\n","    random.seed()\n","\n","  def forward(self, waveform):\n","    return np.roll(waveform, int(random.randint(-self.max_shift, self.max_shift)))\n","\n","transformations.append(AddShift(max_shift = 1600))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rhL_hEqhncPE"},"source":["### Stretch\n","We squeeze the audio by a random factor.\n","\n","If you wish to perform a stretch, decrease `stretch_min` and increase `req_length` under `PadAudio()` accordingly.\n","\n","Padding must increase to accomodate for the amount of stretch, if not, the input Tensor to the model will differ in shape or size."]},{"cell_type":"code","metadata":{"id":"1nJ6lBrcncuE"},"source":["class AddStretch(torch.nn.Module):\n","  def __init__(self, stretch_min = 0.8, stretch_max = 1):\n","    super().__init__()\n","    self.stretch_min = stretch_min\n","    self.stretch_max = stretch_max\n","    random.seed()\n","\n","  def forward(self, waveform):\n","    return librosa.effects.time_stretch(waveform,random.uniform(self.stretch_min, self.stretch_max))\n","\n","transformations.append(AddStretch(stretch_min = 1, stretch_max = 1.3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lJmeX-cvp0uK"},"source":["### Converting Numpy back to Tensor"]},{"cell_type":"code","metadata":{"id":"DrfX0JFAp1LV"},"source":["class toTensor(torch.nn.Module):\n","    def __init__(self, log_offset = 1e-6):\n","        super().__init__()\n","\n","    def forward(self, np_array):\n","        return torch.from_numpy(np_array).type(torch.float32);\n","\n","transformations.append(toTensor())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OQdaXXVmAY2"},"source":["### Padding Audio\n","The model takes in a Tensor as an input, in order to insert our Tensors into the model, we have to ensure that the Tensors have the same size or shape. The sample length varies across audio clips.\n","\n","To ensure constant sample lengths, we can pad the audio clips to a maximum sample length of 16000. (16000 sample length is equal to 1 second at 16,000 Hz sampling rate)\n","\n","We will pad audio clips, which are less than 1 second, with parts of itself."]},{"cell_type":"code","metadata":{"id":"EPCilpvUp19u"},"source":["audio_lens = []\n","for i in range(len(train_set)):\n","  audio_lens.append(train_set[i][1].size(1))\n","\n","print('Max Sample Length:', max(audio_lens))\n","print('Min Sample Length:', min(audio_lens))\n","\n","class PadAudio(torch.nn.Module):\n","  def __init__(self, req_length = 16000):\n","    super().__init__()\n","    self.req_length = req_length\n","\n","  def forward(self, waveform):\n","    while waveform.size(0) < self.req_length:\n","      waveform = torch.cat((waveform, waveform[:self.req_length - waveform.size(0)]), axis=0)\n","    return waveform\n","\n","transformations.append(PadAudio())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CD5HBKjCA3m"},"source":["## Features"]},{"cell_type":"markdown","metadata":{"id":"hb3zuan1yPmQ"},"source":["In this classification example, instead of using the raw waveform of the audio clips, we will craft handmade audio features known as melspectrograms instead.\n","\n","For an in-depth explanation of what a melspectrogram is, I would highly recommend reading this article [here](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53).\n","\n","In short, a melspectrogram is a way to represent an audio signal’s loudness as it varies over time at different frequencies, while scaled to how humans perceive sound. (We can easily tell the difference between 500 and 1000 Hz, but we can't between 10,000 and 10,500 Hz.)\n","\n","![pic](https://i.ibb.co/WDsqsfb/melspectrogram.png)\n","\n","\n","TorchAudio has an in-built method that can help us with this transformation. We shall then apply log scaling."]},{"cell_type":"code","metadata":{"id":"jlN83ccnJ8It"},"source":["# We define our own log transformation here\n","class LogMelTransform(torch.nn.Module):\n","\n","    def __init__(self, log_offset = 1e-6):\n","        super().__init__()\n","        self.log_offset = log_offset\n","\n","    def forward(self, melspectrogram):\n","        return torch.log(melspectrogram + self.log_offset)\n","\n","transformations.append(MelSpectrogram(sample_rate = 16000, n_mels = 128))\n","transformations.append(LogMelTransform())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MeCYS50Awl3A"},"source":["## Features for Evaluation"]},{"cell_type":"markdown","metadata":{"id":"J1Vy4CFe9Mmi"},"source":["We will require a transformation without any form of augmentation to validate and predict on new audios"]},{"cell_type":"code","metadata":{"id":"PjKZIKpgwkev"},"source":["eval_transformations = []\n","\n","eval_transformations.append(PadAudio())\n","eval_transformations.append(MelSpectrogram(sample_rate = 16000, n_mels = 128)) # CHANGE: input size to 256, edit: back to 128 so model is scalable\n","eval_transformations.append(LogMelTransform())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E3_rz0d4Khpn"},"source":["## Spectogram Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"S174be-LxXir"},"source":["We will do a simple data augmentation process in order to increase the variations in our dataset.\n","\n","In the audio domain, the augmentation technique known as [SpecAugment](https://arxiv.org/abs/1904.08779) is often used. It makes use of 3 steps:\n","- Time Warp (warps the spectrogram to the left or right)\n","- Frequency Masking (randomly masks a range of frequencies)\n","- Time Masking (randomly masks a range of time)\n","\n","![specaugment pic](https://drive.google.com/uc?export=view&id=1C085-PlXVhjzh4kzCy869VHRGwC3aDHJ)"]},{"cell_type":"code","metadata":{"id":"omm4z5GsJ-wS"},"source":["# Let's extend the list of transformations with the augmentations\n","transformations.append(TimeMasking(time_mask_param = 10)) \n","transformations.append(FrequencyMasking(freq_mask_param = 16)) \n","transformations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXH-8OJZW5EY"},"source":["#Let's visualise the changes we have so far\n","filename, waveform, sample_rate, label_id = train_set[0]\n","\n","for transform in transformations:\n","  waveform = transform(waveform)\n","plt.plot(waveform.t());"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q9hhlhDDMEos"},"source":["## Data Loaders"]},{"cell_type":"markdown","metadata":{"id":"HgYLGBo69PKk"},"source":["Let's now set up our data loaders so that we can streamline the batch loading of data for our model training later on. "]},{"cell_type":"code","metadata":{"id":"KEJ5I9rNN34D"},"source":["BATCH_SIZE = 8\n","NUM_WORKERS = 4\n","PIN_MEMORY = True if device == 'cuda' else False\n","\n","def train_collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # filename, waveform, sample_rate, label\n","\n","    tensors, targets, filenames = [], [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for filename, waveform, sample_rate, label in batch:\n","        # apply transformations\n","        for transform in transformations:\n","            waveform = transform(waveform)\n","        waveform = waveform.squeeze().T\n","        tensors += [waveform]\n","        targets += [labels_to_indices[label]]\n","        filenames += [filename]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = torch.stack(tensors)\n","    targets = torch.LongTensor(targets)\n","\n","    return (tensors, targets, filenames)\n","\n","def eval_collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # filename, waveform, sample_rate, label\n","\n","    tensors, targets, filenames = [], [], []\n","\n","    # Gather in lists, and encode labels as indices\n","    for filename, waveform, sample_rate, label in batch:\n","        # apply transformations\n","        for transform in eval_transformations:\n","            waveform = transform(waveform)\n","        waveform = waveform.squeeze().T\n","        tensors += [waveform]\n","        targets += [labels_to_indices[label]]\n","        filenames += [filename]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = torch.stack(tensors)\n","    targets = torch.LongTensor(targets)\n","    filenames += [filename]\n","\n","    return (tensors, targets, filenames)\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    drop_last=False,\n","    collate_fn=train_collate_fn,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=eval_collate_fn,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3OWoxpWcM3D4"},"source":["## Setting up the LSTM-RNN Model"]},{"cell_type":"code","metadata":{"id":"OQWw_iedO1wa"},"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes, device, classes=None):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, num_classes)\n","        self.device = device\n","        self.classes = classes\n","\n","    def forward(self, x):\n","        # Set initial hidden and cell states\n","        batch_size = x.size(0)\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n","        \n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))  # shape = (batch_size, seq_length, hidden_size)\n","        \n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","    def predict(self, x):\n","        '''Predict one label from one sample's features'''\n","        # x: feature from a sample, LxN\n","        #   L is length of sequency\n","        #   N is feature dimension\n","        x = torch.tensor(x[np.newaxis, :], dtype=torch.float32)\n","        x = x.to(self.device)\n","        outputs = self.forward(x)\n","        _, predicted = torch.max(outputs.data, 1)\n","        predicted_index = predicted.item()\n","        return predicted_index\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJPXndbLVSRT"},"source":["# initialise dataset object for test set\n","test_set = CustomSpeechDataset(path='test_dataset', typ='test')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Cqwt6wCV9lg"},"source":["# define test collate function and set up test loader\n","def test_collate_fn(batch):\n","\n","    # A data tuple has the form:\n","    # filename, waveform, sample_rate\n","\n","    tensors, filenames = [], []\n","\n","    # Gather in lists\n","    for filename, waveform, sample_rate in batch:\n","        # apply transformations\n","        for transform in eval_transformations:\n","            waveform = transform(waveform)\n","        waveform = waveform.squeeze().T\n","        tensors += [waveform]\n","        filenames += [filename]\n","\n","    # Group the list of tensors into a batched tensor\n","    tensors = torch.stack(tensors)\n","\n","    return (tensors, filenames)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_set,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    drop_last=False,\n","    collate_fn=test_collate_fn,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1aywpoys1BZR"},"source":["# initialize the model class\n","model = RNN(input_size=128, hidden_size=256, num_layers=2, num_classes=len(labels), device=device, classes=labels).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","optimizer.zero_grad()\n","\n","# scheduler that decays by 0.1 every 3 epochs\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9) \n","\n","# define number of epochs\n","num_epochs = 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"00v-tOdkrAmu"},"source":["for epoch in range(1,num_epochs+1):\n","\n","  # training steps\n","  model.train()\n","  count_correct, count_total = 0, 0\n","  for idx, (features, targets, filenames) in enumerate(train_loader):\n","\n","    features = features.to(device)\n","    targets = targets.to(device)\n","\n","    # forward pass\n","    outputs = model(features)\n","    loss = criterion(outputs, targets)\n","\n","    # backward pass\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # training results\n","    _, argmax = torch.max(outputs, 1)\n","    count_correct += (targets == argmax.squeeze()).sum().item()\n","    count_total += targets.size(0)\n","\n","  train_acc = count_correct / count_total\n","  \n","  # evaluation steps\n","  model.eval()\n","  count_correct, count_total = 0, 0\n","  with torch.no_grad():\n","    for idx, (features, targets, filenames) in enumerate(valid_loader):\n","\n","      features = features.to(device)\n","      targets = targets.to(device)\n","\n","      # forward pass\n","      val_outputs = model(features)\n","      val_loss = criterion(val_outputs, targets)\n","\n","      # validation results\n","      _, argmax = torch.max(val_outputs, 1)\n","      count_correct += (targets == argmax.squeeze()).sum().item()\n","      count_total += targets.size(0)\n","\n","  # print results\n","  valid_acc = count_correct / count_total\n","  print('Epoch [{}/{}], Train loss = {:.4f}, Train accuracy = {:.2f}, Valid loss = {:.4f}, Valid accuracy = {:.2f}' \n","        .format(epoch, num_epochs, loss.item(), 100*train_acc, val_loss.item(), 100*valid_acc))\n","  \n","  scheduler.step()\n","\n","  # # Early stoppage, if required\n","  # if 100*valid_acc >= 90.0:\n","  #     break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-9T9ZEsteo8"},"source":["save_path = os.path.join(base_folder, 'audio_classification_lstm.pt') # Change this path to your desired directory\n","torch.save(model.state_dict(), save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUGlO1lFRQQd"},"source":["## Test Set"]},{"cell_type":"code","metadata":{"id":"wU8vzrvgRUhT"},"source":["load_path = os.path.join(base_folder, 'audio_classification_lstm.pt') # Change this path to your desired directory\n","model.load_state_dict(torch.load(load_path, map_location=device))\n","\n","# pass test set through the RNN model\n","model.eval()\n","pred_list, filename_list = [], []\n","with torch.no_grad():\n","  for idx, (features, filenames) in enumerate(test_loader):\n","\n","    features = features.to(device)\n","\n","    # forward pass\n","    outputs = model(features)\n","\n","    # validation results\n","    _, argmax = torch.max(outputs, 1)\n","    pred_list += argmax.cpu().tolist()\n","    filename_list += filenames"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeGHrPzIa9_7"},"source":["## Converting prediction results into a CSV file"]},{"cell_type":"code","metadata":{"id":"hGPpF_F7QX98"},"source":["pred_path = os.path.join(base_folder, 'prediction.csv') # Change this path to your desired directory\n","\n","result_tuple = list(zip(filename_list, pred_list))\n","submission = pd.DataFrame(result_tuple, columns=['filename', 'pred'])\n","submission = submission.sort_values('filename').reset_index(drop=True)\n","submission['label'] = submission['pred'].apply(lambda x: labels[x])\n","submission[['filename', 'label']].head()\n","submission[['filename', 'label']].to_csv(pred_path, header=None, index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W05Cxt9X8Cys"},"source":["Predictions `prediction.csv` or the saved model `audio_classification_lstm.pt` can be found in `base_folder` defined above."]},{"cell_type":"code","metadata":{"id":"hVR8trKFYY9v"},"source":[""],"execution_count":null,"outputs":[]}]}